Music Assistant v4 - 개선 개발 계획서

프로젝트명: Music Assistant v4 - 리듬 양자화 및 악보 생성 고도화
핵심 목표: v3의 안정적인 아키텍처를 기반으로, 사용자가 직접 BPM과 박자 그리드를 설정하여 물리적인 온셋(onset) 데이터를 음악적으로 의미 있는 리듬 정보로 변환(양자화)하는 기능을 구현하고, 이를 통해 최종 악보 생성의 정확도를 극대화합니다.

프로젝트 개요

각 단계(Phase)는 독립적이고 완전한 실행 가능한 결과물을 목표로 하며, 모든 신규 백엔드 작업은 backend_v4 디렉토리에서 진행됩니다.

Phase 1: 비동기 아키텍처 기반 구축 (v3 계승 및 안정화)

목표: v3에서 구축된 비동기 처리 아키텍처를 계승하여, v4의 안정적인 백엔드 기반을 마련합니다.

백엔드 (backend_v4)

신규 작업 환경 구성

- [x] 기존 버전에 영향을 주지 않도록 backend_v4 디렉토리 생성

기술 스택 및 설정

- [x] FastAPI, Celery, Redis를 사용한 비동기 아키텍처 구성 (v3와 동일)

비동기 API 엔드포인트 설계

- [x] POST /api/v4/analysis: 오디오 파일과 분석 옵션을 받아 작업을 Celery 큐에 등록하고, 즉시 task_id를 반환
- [x] GET /api/v4/analysis/{task_id}: task_id를 사용하여 작업 상태(대기, 진행 중, 성공, 실패) 및 결과를 조회

기본 분석 작업 구현

- [x] Celery 워커에서 실행될 기본 오디오 분석 작업(음고, 기본 온셋) 구현
- [x] 분석 완료 시 결과를 Redis 결과 백엔드에 저장

실행 및 테스트

- [x] backend_v4의 FastAPI 서버와 Celery 워커를 각각 실행
- [x] 긴 오디오 파일을 업로드해도 500 오류 없이 task_id가 즉시 반환되는지 확인
- [x] 폴링(Polling) 엔드포인트를 통해 작업 상태가 정상적으로 조회되는지 확인

결과물

- [x] 대용량 오디오 파일도 안정적으로 처리할 수 있는 비동기 방식의 backend_v4 API 서버

Phase 2: 고급 온셋 탐지 및 민감도 조절 기능 (v3 계승)

목표: v3에서 구현된 고급 온셋 탐지 로직과 민감도 조절 기능을 v4 환경에 맞게 이식하고 안정화합니다.

백엔드 (backend_v4)

핵심 로직 개선

- [x] Celery 작업 내 온셋 탐지 로직을 librosa.onset.onset_strength와 librosa.util.peak_pick 조합으로 유지 1
- [x] 스펙트럼 플럭스(Spectral Flux) 기반 알고리즘을 통해 음악적으로 의미 있는 온셋 추출 2

API 엔드포인트 확장

- [x] POST /api/v4/analysis 엔드포인트가 온셋 탐지 민감도 조절 파라미터(delta, wait)를 받도록 수정

동적 파라미터 적용

- [x] 사용자가 전달한 delta, wait 값을 Celery 작업으로 넘겨 librosa.util.peak_pick 함수에 동적으로 적용

실행 및 테스트

- [x] 동일한 오디오 파일에 대해 delta 값을 다르게 설정하여 API를 호출하고, 온셋 탐지 결과 개수가 의도대로 변경되는지 확인

결과물

- [x] 정확도가 향상되고, 사용자가 API를 통해 민감도를 직접 제어할 수 있는 고도화된 온셋 분석 엔진

Phase 3: 리듬 양자화 및 그리드 설정 기능 구현 (v4 핵심 기능)

목표: 사용자가 직접 입력한 BPM과 그리드 설정을 바탕으로, 물리적 시간축의 온셋 데이터를 음악적 시간축(마디, 박자)에 정렬하는 리듬 양자화(Rhythm Quantization) 기능을 구현합니다.

백엔드 (backend_v4)

API 엔드포인트 확장

- [x] POST /api/v4/analysis 엔드포인트가 리듬 분석을 위한 추가 파라미터를 받도록 확장:
  - bpm (float, optional): 사용자가 직접 입력하는 곡의 템포. 미입력 시 자동 추정.
  - grid_resolution (string, optional): 양자화의 기준이 될 박자 그리드 해상도 (예: '1/4', '1/8', '1/16', '1/8t' (8분음표 셋잇단음표)). 기본값은 '1/16'.

리듬 양자화 로직 구현

- [x] Celery 작업 내에 다음 로직을 추가:
  1. 템포 결정: 사용자가 bpm을 입력하면 해당 값을 사용. 입력이 없으면 librosa.beat.beat_track을 통해 템포를 자동 추정.4
  2. 비트 그리드 생성: 결정된 템포(BPM)를 기준으로, 오디오 전체에 걸쳐 각 비트(beat)가 발생하는 시간(초) 배열을 계산.6
  3. 양자화 그리드 생성: grid_resolution 설정에 따라 비트 그리드를 더 세분화하여 최종 양자화 그리드를 생성.
  4. 온셋 양자화: Phase 2에서 탐지된 온셋(onsets)들을 가장 가까운 양자화 그리드 선에 '스냅(snap)'하여 음악적 시작 시간을 계산.8
  5. 음표 길이 계산: 양자화된 시작 시간을 기준으로 각 음표의 음악적 길이(예: 1.0비트 = 4분음표, 0.5비트 = 8분음표)를 계산.

프론트엔드 (Next.js) - 수정 필요

- [x] 분석 요청 UI에 BPM을 입력할 수 있는 숫자 입력 필드 추가.
- [x] 그리드 해상도를 선택할 수 있는 드롭다운 메뉴(e.g., '1/8', '1/16', 'Triplet') 추가.
- [x] 사용자가 입력한 값을 POST /api/v4/analysis 요청 시 함께 전송.

실행 및 테스트

- [x] 동일한 파일에 대해 BPM을 직접 입력한 경우와 자동 추정한 경우의 결과가 어떻게 다른지 비교.
- [x] grid_resolution을 변경하며 요청했을 때, 반환되는 양자화된 온셋 데이터가 의도대로 변경되는지 확인.

결과물

- [x] 사용자의 의도에 따라 리듬을 해석하고 정렬할 수 있는 지능형 리듬 양자화 엔진.

Phase 4: 프론트엔드 시각화를 위한 데이터 고도화

목표: Phase 3에서 생성된 음악적 그리드와 양자화된 노트 정보를 포함하여, 프론트엔드에서 더욱 풍부하고 상호작용적인 차트를 렌더링할 수 있도록 최종 JSON 데이터를 확장합니다.

백엔드 (backend_v4)

최종 출력 JSON 구조 확장

- [x] GET /api/v4/analysis/{task_id}가 작업 성공 시 반환할 최종 JSON 데이터 구조를 다음과 같이 확장:
  - [x] 파형 데이터: waveform { data, times }
  - [x] 음고 윤곽선: pitch_contour
  - [x] 온셋 마커:
    - onsets: 양자화 이전의 원본 온셋 시간(초) 배열.
    - quantized_onsets: 양자화된 온셋 시간(초) 배열.
  - [x] 리듬 정보:
    - beat_grid: 계산된 비트 그리드의 시간(초) 값 배열.
    - quantized_notes: 각 음표의 상세 정보를 담은 객체 배열.
      - pitch_hz: 음고 (Hz)
      - start_time_sec: 양자화된 시작 시간 (초)
      - duration_sec: 계산된 지속 시간 (초)
      - start_time_beat: 음악적 시작 시간 (비트 단위)
      - duration_beat: 음악적 길이 (비트 단위, 예: 1.0, 0.5)
  - [x] 분석 메타데이터: 분석에 사용된 delta, bpm, grid_resolution 등 주요 파라미터 값.

프론트엔드 (Next.js) - 수정 필요

- [x] Canvas 기반 시각화 라이브러리에서 확장된 JSON 데이터를 사용하도록 로직 수정.
- [x] 차트 배경에 beat_grid 데이터를 사용하여 수직 박자선을 렌더링.
- [x] quantized_notes 데이터를 사용하여 정확한 위치와 길이의 음표(사각형)를 렌더링.
- [x] 원본 온셋과 양자화된 온셋을 비교하여 볼 수 있는 토글 기능 구현.

실행 및 테스트

- [x] 분석 성공 후 반환되는 JSON 객체가 확장된 구조와 모든 필수 데이터를 포함하는지 확인.
- [x] 프론트엔드에서 박자 그리드와 양자화된 노트가 정확하게 시각화되는지 검증.

결과물

- [x] 음악적 맥락(박자, 그리드)을 포함하여 시각화에 최적화된 고도화된 JSON 데이터 제공 기능.

Phase 5: 음악 악보 생성 기능 (고도화)

목표: Phase 3에서 생성된 정확한 리듬 데이터를 활용하여, 사람이 읽기 좋은 고품질의 MusicXML 악보를 생성합니다.

백엔드 (backend_v4)

악보 변환 로직 고도화

- [x] music21 라이브러리 통합.
- [x] Phase 4의 quantized_notes 배열을 사용하여 music21의 Note 객체 생성.
  - pitch_hz -> note.pitch (Hz를 MIDI 번호로 변환)
  - start_time_beat -> note.offset (시작 위치)
  - duration_beat -> note.quarterLength (음표 길이)
- [x] 이 방식은 물리적 시간(초)을 직접 변환하던 v3에 비해 훨씬 정확하고 음악적인 악보를 생성.

신규 API 엔드포인트 설계

- [x] GET /api/v4/analysis/{task_id}/musicxml: 분석이 완료된 작업에 대해 MusicXML 형식의 악보 데이터를 문자열로 반환하는 엔드포인트 생성.

프론트엔드 (Next.js) - 연동 가이드

- [x] MusicXML 다운로드 버튼 구현 (브라우저에서 직접 다운로드 가능)
- [x] 다운로드된 파일을 MuseScore, Finale 등 악보 소프트웨어에서 열 수 있도록 안내 추가

실행 및 테스트

- [x] 생성된 MusicXML 파일을 MuseScore나 OSMD 뷰어에서 열었을 때, 원본 멜로디의 리듬이 정확하게 악보로 표기되는지 확인.
- [x] 셋잇단음표 등 복잡한 리듬이 포함된 곡에 대해 양자화 및 악보 생성이 올바르게 동작하는지 검증.

결과물

- [x] 양자화된 리듬 정보를 기반으로 생성된, 정확도 높은 MusicXML 악보 데이터 제공 기능.
